{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47980480-d005-41a6-a165-aaaf2aea532f",
   "metadata": {},
   "source": [
    "## Step 1: Installation and start spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "195c06b1-e052-49e8-87ac-d82cbd4c9bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (3.5.7)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de3d7ac-f2bb-499e-99a6-7f52432acb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (1.24.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vedant\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: pip is being invoked by an old script wrapper. This will fail in a future version of pip.\n",
      "Please see https://github.com/pypa/pip/issues/5599 for advice on fixing the underlying issue.\n",
      "To avoid this problem you can invoke Python with '-m pip' instead of running pip directly.\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f5cf134-334f-415c-8f82-2690a8c606a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n",
      "2.0.3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b872c351-6c2e-4ada-8c14-bcc8a5c5dcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.7\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop any existing Spark session first\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create a clean Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AI Impact Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d4b6b-e8ff-4fcc-a876-9eecafb721f1",
   "metadata": {},
   "source": [
    "## Step 2: Manual Data Splitting Across Nodes\n",
    "Split the dataset into three parts: one for master, one for worker 1, and one for worker 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2786f30a-6c10-4002-8a91-e752ef80f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets split successfully:\n",
      "Master: 1020 rows\n",
      "Worker1: 990 rows\n",
      "Worker2: 990 rows\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "full_df = pd.read_csv(\"AI_Impact_on_Jobs_2030.csv\")\n",
    "\n",
    "df_master = full_df.sample(frac=0.34, random_state=42)\n",
    "remaining = full_df.drop(df_master.index)\n",
    "df_worker1 = remaining.sample(frac=0.5, random_state=42)\n",
    "df_worker2 = remaining.drop(df_worker1.index)\n",
    "\n",
    "df_master.to_csv(\"AI_Impact_on_Jobs_2030_master.csv\", index=False)\n",
    "df_worker1.to_csv(\"AI_Impact_on_Jobs_2030_worker1.csv\", index=False)\n",
    "df_worker2.to_csv(\"AI_Impact_on_Jobs_2030_worker2.csv\", index=False)\n",
    "\n",
    "print(\"Datasets split successfully:\")\n",
    "print(\"Master:\", len(df_master), \"rows\")\n",
    "print(\"Worker1:\", len(df_worker1), \"rows\")\n",
    "print(\"Worker2:\", len(df_worker2), \"rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1585a6-fb93-4352-8c2e-425cf2ad1793",
   "metadata": {},
   "source": [
    "## Step 3: Load Dataset on Each Node\n",
    "Load the CSV partitions into Spark DataFrames simulating master and workers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5343994f-a8ad-4bab-89e7-c84cc3c5db72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Job_Title: string (nullable = true)\n",
      " |-- Average_Salary: integer (nullable = true)\n",
      " |-- Years_Experience: integer (nullable = true)\n",
      " |-- Education_Level: string (nullable = true)\n",
      " |-- AI_Exposure_Index: double (nullable = true)\n",
      " |-- Tech_Growth_Factor: double (nullable = true)\n",
      " |-- Automation_Probability_2030: double (nullable = true)\n",
      " |-- Risk_Category: string (nullable = true)\n",
      " |-- Skill_1: double (nullable = true)\n",
      " |-- Skill_2: double (nullable = true)\n",
      " |-- Skill_3: double (nullable = true)\n",
      " |-- Skill_4: double (nullable = true)\n",
      " |-- Skill_5: double (nullable = true)\n",
      " |-- Skill_6: double (nullable = true)\n",
      " |-- Skill_7: double (nullable = true)\n",
      " |-- Skill_8: double (nullable = true)\n",
      " |-- Skill_9: double (nullable = true)\n",
      " |-- Skill_10: double (nullable = true)\n",
      "\n",
      "+-------------------+--------------+----------------+---------------+-----------------+------------------+---------------------------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "|          Job_Title|Average_Salary|Years_Experience|Education_Level|AI_Exposure_Index|Tech_Growth_Factor|Automation_Probability_2030|Risk_Category|Skill_1|Skill_2|Skill_3|Skill_4|Skill_5|Skill_6|Skill_7|Skill_8|Skill_9|Skill_10|\n",
      "+-------------------+--------------+----------------+---------------+-----------------+------------------+---------------------------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "|      UX Researcher|        146298|               6|     Bachelor's|             0.64|              1.04|                       0.54|       Medium|   0.68|   0.17|    0.3|   0.74|   0.16|   0.34|   0.67|   0.94|   0.08|    0.52|\n",
      "|Construction Worker|        134042|              12|    High School|             0.79|              1.41|                       0.75|         High|   0.52|   0.84|   0.11|   0.79|   0.18|   0.81|   0.61|   0.19|   0.47|    0.68|\n",
      "|  Financial Analyst|         64118|               5|            PhD|             0.28|              0.94|                       0.45|       Medium|   0.74|    0.6|   0.65|   0.21|   0.33|   0.02|   0.11|   0.27|   0.07|     0.6|\n",
      "|           Mechanic|        105831|              22|       Master's|              0.8|              0.83|                       0.42|       Medium|   0.23|   0.13|   0.26|   0.36|   0.67|   0.06|   0.21|   0.81|   0.15|    0.34|\n",
      "|     Data Scientist|         73489|               2|       Master's|             0.32|              0.93|                       0.34|       Medium|    1.0|   0.76|   0.62|   0.59|    0.5|   0.26|   0.23|   0.55|   0.07|    0.07|\n",
      "+-------------------+--------------+----------------+---------------+-----------------+------------------+---------------------------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows combined: 3000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"AI Impact Analysis\")\n",
    "    .master(\"local[*]\")  # local[*] works in notebook for testing\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "\n",
    "df_master = spark.read.csv(\"AI_Impact_on_Jobs_2030_master.csv\", header=True, inferSchema=True)\n",
    "df_worker1 = spark.read.csv(\"AI_Impact_on_Jobs_2030_worker1.csv\", header=True, inferSchema=True)\n",
    "df_worker2 = spark.read.csv(\"AI_Impact_on_Jobs_2030_worker2.csv\", header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df_all = df_master.union(df_worker1).union(df_worker2)\n",
    "\n",
    "# Preview combined DataFrame\n",
    "df_all.printSchema()\n",
    "df_all.show(5)\n",
    "print(\"Total rows combined:\", df_all.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef431a0-7155-4cab-85ee-086a66455dd5",
   "metadata": {},
   "source": [
    "## Step 4: Data Partitioning and Spark SQL Processing\n",
    "Repartition the data, clean it, and create features using SQL queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa699e5a-5a21-4cb5-bf22-5e448b82a6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|AI_Exposure_Index|cnt|\n",
      "+-----------------+---+\n",
      "|0.0              |12 |\n",
      "|0.01             |38 |\n",
      "|0.02             |28 |\n",
      "|0.03             |28 |\n",
      "|0.04             |28 |\n",
      "|0.05             |26 |\n",
      "|0.06             |30 |\n",
      "|0.07             |30 |\n",
      "|0.08             |27 |\n",
      "|0.09             |27 |\n",
      "|0.1              |33 |\n",
      "|0.11             |27 |\n",
      "|0.12             |28 |\n",
      "|0.13             |27 |\n",
      "|0.14             |27 |\n",
      "|0.15             |24 |\n",
      "|0.16             |27 |\n",
      "|0.17             |32 |\n",
      "|0.18             |32 |\n",
      "|0.19             |40 |\n",
      "+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "Total rows after repartitioning and cleaning: 3000\n",
      "+-----------------+--------------+----------------+---------------+-----------------+------------------+---------------------------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "|Job_Title        |Average_Salary|Years_Experience|Education_Level|AI_Exposure_Index|Tech_Growth_Factor|Automation_Probability_2030|Risk_Category|Skill_1|Skill_2|Skill_3|Skill_4|Skill_5|Skill_6|Skill_7|Skill_8|Skill_9|Skill_10|\n",
      "+-----------------+--------------+----------------+---------------+-----------------+------------------+---------------------------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "|AI Engineer      |133496        |13              |High School    |0.34             |0.68              |0.05                       |Low          |0.47   |0.69   |0.23   |0.13   |0.89   |0.28   |0.22   |0.86   |0.82   |0.45    |\n",
      "|AI Engineer      |30077         |25              |High School    |0.91             |0.82              |0.2                        |Low          |0.68   |0.45   |0.71   |0.9    |0.62   |0.54   |0.44   |0.58   |0.36   |0.39    |\n",
      "|Customer Support |71164         |25              |PhD            |0.91             |1.23              |0.86                       |High         |0.03   |0.35   |0.35   |0.54   |0.26   |0.27   |0.62   |0.94   |0.66   |0.67    |\n",
      "|Marketing Manager|78145         |11              |Bachelor's     |0.91             |1.23              |0.69                       |Medium       |0.89   |0.22   |0.76   |0.05   |0.82   |0.6    |0.96   |0.96   |0.05   |0.16    |\n",
      "|Teacher          |39587         |5               |Bachelor's     |0.54             |1.21              |0.13                       |Low          |0.47   |0.82   |0.46   |0.36   |0.49   |0.83   |0.34   |0.17   |0.71   |0.83    |\n",
      "+-----------------+--------------+----------------+---------------+-----------------+------------------+---------------------------+-------------+-------+-------+-------+-------+-------+-------+-------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Create temp view for SQL queries\n",
    "df_all.createOrReplaceTempView(\"ai_jobs\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT AI_Exposure_Index, COUNT(*) AS cnt\n",
    "FROM ai_jobs\n",
    "GROUP BY AI_Exposure_Index\n",
    "ORDER BY AI_Exposure_Index\n",
    "\"\"\").show(20, truncate=False)\n",
    "\n",
    "# Repartition by AI_Exposure_Index for parallel processing\n",
    "df_all = df_all.repartition(8, \"AI_Exposure_Index\").cache()\n",
    "\n",
    "#  Drop rows with nulls in columns used for ML\n",
    "essential_cols = [\n",
    "    \"Risk_Category\", \"Education_Level\", \"Job_Title\", \"Average_Salary\",\n",
    "    \"Years_Experience\", \"AI_Exposure_Index\", \"Tech_Growth_Factor\",\n",
    "    \"Automation_Probability_2030\"\n",
    "] + [f\"Skill_{i}\" for i in range(1, 11)]\n",
    "df_all = df_all.dropna(subset=essential_cols)\n",
    "\n",
    "total_rows = df_all.count()\n",
    "print(\"Total rows after repartitioning and cleaning:\", total_rows)\n",
    "\n",
    "df_all.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9b14d3-5dd8-43a4-ac47-529c5b9d9420",
   "metadata": {},
   "source": [
    "## Step 5: Machine Learning Pipeline\n",
    "Build a Spark MLlib pipeline with feature assembly, scaling, and logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c2ac1f2-6eb0-4050-9219-6718635b7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed Model Training Time: 16.78 seconds\n",
      "GLOBAL MODEL F1: 0.995045616843293\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# -------------------------------\n",
    "# Feature & Label Preparation\n",
    "# -------------------------------\n",
    "\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Risk_Category\",\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "education_indexer = StringIndexer(\n",
    "    inputCol=\"Education_Level\",\n",
    "    outputCol=\"education_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "job_indexer = StringIndexer(\n",
    "    inputCol=\"Job_Title\",\n",
    "    outputCol=\"job_idx\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"education_idx\", \"job_idx\"],\n",
    "    outputCols=[\"education_vec\", \"job_vec\"]\n",
    ")\n",
    "\n",
    "numeric_features = [\n",
    "    \"Average_Salary\",\n",
    "    \"Years_Experience\",\n",
    "    \"AI_Exposure_Index\",\n",
    "    \"Tech_Growth_Factor\",\n",
    "    \"Automation_Probability_2030\"\n",
    "] + [f\"Skill_{i}\" for i in range(1, 11)]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=numeric_features + [\"education_vec\", \"job_vec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=50\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    label_indexer,\n",
    "    education_indexer,\n",
    "    job_indexer,\n",
    "    encoder,\n",
    "    assembler,\n",
    "    lr\n",
    "])\n",
    "\n",
    "# -------------------------------\n",
    "# Train-Test Split\n",
    "# -------------------------------\n",
    "\n",
    "train_df, test_df = df_all.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# -------------------------------\n",
    "# Measure Training Time\n",
    "# -------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "end_time = time.time()\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# -------------------------------\n",
    "# Prediction & Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "pred = model.transform(test_df)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "print(f\"Distributed Model Training Time: {training_time:.2f} seconds\")\n",
    "print(\"GLOBAL MODEL F1:\", evaluator.evaluate(pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c4cfef-cbcc-4b3b-bf36-5630a6a04576",
   "metadata": {},
   "source": [
    "### Observation\r\n",
    "The distributed machine learning model trained on the unified dataset achieved a very high F1-score of **0.995**, indicating excellent classification performance.  \r\n",
    "Although the training time was **16.78 seconds**, it is expected due to distributed coordination and data shuffling across nodes, which trades higher computation time for better overall model accuracy.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0788d123-1cb7-4bc3-8c00-6db2271ab306",
   "metadata": {},
   "source": [
    "### Step 6: Resource Management & Monitoring\r\n",
    "\r\n",
    "- Spark executor memory, cores, and shuffle partitions can be configured for optimized performance.\r\n",
    "- In local mode, the default settings are used, but on a real cluster you can adjust:\r\n",
    "  - `spark.executor.memory` for executor RAM\r\n",
    "  - `spark.executor.cores` for CPU cores per executor\r\n",
    "  - `spark.sql.shuffle.partitions` for shuffle parallelism\r\n",
    "- Monitor resource usage via the **Spark Web UI** at `http://localhost:4040`.\r\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28b66e7-5c84-4778-ab46-24f8792c21aa",
   "metadata": {},
   "source": [
    "### Step 7: Performance Monitoring\n",
    "- Monitor Spark cluster performance using the **Spark Web UI** at `http://localhost:4040`.\n",
    "- Key metrics to check:\n",
    "  - **Task distribution** across worker nodes\n",
    "  - **Memory usage** per executor\n",
    "  - **CPU utilization** per node\n",
    "- Helps identify bottlenecks and optimize resource allocation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbd9284-0487-4376-8702-f5673f07a25d",
   "metadata": {},
   "source": [
    "### Step 8: Tuning the Machine Learning Model\n",
    "- Fine-tune the ML model by adjusting **hyperparameters** (e.g., regularization, maxIter, tree depth).\n",
    "- Use **cross-validation** to evaluate different configurations and select the best model.\n",
    "- Goal: Improve model accuracy, precision, or F1-score on distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657576e6-f270-4a10-bfb2-8a96ff1ccf01",
   "metadata": {},
   "source": [
    "### Part 2 — Step 1: Simultaneous ML Jobs on Each Node\r\n",
    "- Train separate machine learning models on the Master and Worker node datasets.\r\n",
    "- Each node handles its own portion of the dataset independently.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e313b72e-d193-4575-952a-6fba3920e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master node dataset count: 1006\n",
      "Worker 1 dataset count: 965\n",
      "Worker 2 dataset count: 1029\n",
      "Master Node Training Time: 9.43 seconds\n",
      "F1 Score for Master Node: 0.9572\n",
      "\n",
      "Worker 1 Training Time: 8.77 seconds\n",
      "F1 Score for Worker 1: 0.9898\n",
      "\n",
      "Worker 2 Training Time: 6.48 seconds\n",
      "F1 Score for Worker 2: 0.9766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- Part 2.1: Prepare Node-Specific Datasets ---\n",
    "\n",
    "# Split df_all roughly into three parts to simulate nodes\n",
    "master_df, worker1_df, worker2_df = df_all.randomSplit([0.33, 0.33, 0.34], seed=42)\n",
    "\n",
    "# Verify counts\n",
    "print(\"Master node dataset count:\", master_df.count())\n",
    "print(\"Worker 1 dataset count:\", worker1_df.count())\n",
    "print(\"Worker 2 dataset count:\", worker2_df.count())\n",
    "\n",
    "# --- Part 2.1: Train Separate ML Models on Each Node ---\n",
    "\n",
    "# Function to train ML pipeline on a node's dataset\n",
    "def train_node_model(node_df, node_name):\n",
    "    # Train/Test split\n",
    "    train_df, test_df = node_df.randomSplit([0.8, 0.2], seed=42)\n",
    "    \n",
    "    # ⏱️ Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Fit the pipeline\n",
    "    model = pipeline.fit(train_df)\n",
    "    \n",
    "    # ⏱️ End timing\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    # Predictions\n",
    "    pred = model.transform(test_df)\n",
    "    \n",
    "    # Evaluate F1\n",
    "    f1_score = evaluator.evaluate(pred)\n",
    "    \n",
    "    print(f\"{node_name} Training Time: {training_time:.2f} seconds\")\n",
    "    print(f\"F1 Score for {node_name}: {f1_score:.4f}\\n\")\n",
    "    \n",
    "    return model, training_time\n",
    "\n",
    "# Train models on each node\n",
    "model_master, time_master = train_node_model(master_df, \"Master Node\")\n",
    "model_worker1, time_worker1 = train_node_model(worker1_df, \"Worker 1\")\n",
    "model_worker2, time_worker2 = train_node_model(worker2_df, \"Worker 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96d246-f7cd-4649-a04d-b6a9de9de570",
   "metadata": {},
   "source": [
    "### Observation\r\n",
    "Each node trained its machine learning model on a smaller, local portion of the dataset, resulting in **shorter training times** compared to the distributed model.  \r\n",
    "While training was faster (6–9 seconds), the F1-scores were **slightly lower and varied across nodes**, showing that models trained on partial data may lose some generalization capability compared to a unified distributed model.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be313104-9b3c-46b6-97a7-fb549f9acf1b",
   "metadata": {},
   "source": [
    "### Part 2 — Step 2: Distributed Machine Learning on Unified Dataset\n",
    "- Train a single machine learning model on the unified dataset (`df_all`) spread across all nodes.\n",
    "- This simulates a distributed ML job using the entire dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08b4e33-ebd9-4f07-bc1b-1c742cf897ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distributed Model Training Time: 7.61 seconds\n",
      "F1 Score for Distributed Model: 0.9950\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# --- Part 2.2: Distributed ML on Unified Dataset ---\n",
    "\n",
    "# Train/Test split on the unified dataset\n",
    "train_df, test_df = df_all.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# ⏱️ Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the same pipeline\n",
    "distributed_model = pipeline.fit(train_df)\n",
    "\n",
    "# ⏱️ End timing\n",
    "end_time = time.time()\n",
    "distributed_training_time = end_time - start_time\n",
    "\n",
    "# Predictions\n",
    "pred_dist = distributed_model.transform(test_df)\n",
    "\n",
    "# Evaluate F1 score\n",
    "f1_dist = evaluator.evaluate(pred_dist)\n",
    "\n",
    "print(f\"Distributed Model Training Time: {distributed_training_time:.2f} seconds\")\n",
    "print(f\"F1 Score for Distributed Model: {f1_dist:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7def3f-4e25-4786-a196-bd0910eb7dcd",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The distributed machine learning model trained on the unified dataset achieved a **high F1-score of 0.995**, demonstrating strong predictive performance.  \n",
    "The training time of **7.61 seconds** shows that distributed processing efficiently utilized cluster resources to train the model on the complete dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2fffdc-d350-471d-921b-ba4f1e1ad45e",
   "metadata": {},
   "source": [
    "### Part 2 — Step 3: Performance Monitoring & Comparison\n",
    "- Monitor and compare model performance across separate node models and the distributed model.\n",
    "- Compare F1 scores and dataset counts to analyze differences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c805d28-7133-4b81-bdc2-510aa03af850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset counts for each node and unified dataset:\n",
      "Master Node: 1006\n",
      "Worker 1: 965\n",
      "Worker 2: 1029\n",
      "Unified Dataset: 3000\n",
      "\n",
      "F1 Scores Comparison:\n",
      "Master Node: 0.9911\n",
      "Worker 1: 0.9979\n",
      "Worker 2: 0.9951\n",
      "Distributed Model: 0.9950\n",
      "\n",
      "Training Time Comparison (seconds):\n",
      "Master Node: 9.43\n",
      "Worker 1: 8.77\n",
      "Worker 2: 6.48\n",
      "Distributed Model: 7.61\n"
     ]
    }
   ],
   "source": [
    "# --- Part 2.3: Compare Models ---\n",
    "\n",
    "print(\"Dataset counts for each node and unified dataset:\")\n",
    "print(f\"Master Node: {master_df.count()}\")\n",
    "print(f\"Worker 1: {worker1_df.count()}\")\n",
    "print(f\"Worker 2: {worker2_df.count()}\")\n",
    "print(f\"Unified Dataset: {df_all.count()}\")\n",
    "\n",
    "print(\"\\nF1 Scores Comparison:\")\n",
    "print(f\"Master Node: {evaluator.evaluate(model_master.transform(master_df)):.4f}\")\n",
    "print(f\"Worker 1: {evaluator.evaluate(model_worker1.transform(worker1_df)):.4f}\")\n",
    "print(f\"Worker 2: {evaluator.evaluate(model_worker2.transform(worker2_df)):.4f}\")\n",
    "print(f\"Distributed Model: {f1_dist:.4f}\")\n",
    "\n",
    "print(\"\\nTraining Time Comparison (seconds):\")\n",
    "print(f\"Master Node: {time_master:.2f}\")\n",
    "print(f\"Worker 1: {time_worker1:.2f}\")\n",
    "print(f\"Worker 2: {time_worker2:.2f}\")\n",
    "print(f\"Distributed Model: {distributed_training_time:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a621a2-ba25-4f11-932c-28dbfb6c7e54",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The unified distributed dataset contains all **3000 records**, while individual nodes process smaller subsets ranging from **965 to 1029 records**.  \n",
    "Node-level models trained faster due to reduced data size but showed **variation in F1-scores**, whereas the distributed model achieved a **consistently high F1-score (0.995)** with balanced training time.  \n",
    "This highlights the trade-off between **faster local training** and **better generalization** achieved through distributed learning.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (Spark)",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
